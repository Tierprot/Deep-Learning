{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tierprot/Deep-Learning/blob/master/homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZa6pOT_YQpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import itertools\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtnrv3uYYdVG",
        "colab_type": "code",
        "outputId": "9ea7b917-c268-4ae2-ff00-744682c9d0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXyDGvaSYiH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab_Notebooks/rnn')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1rEuBoTHPLS",
        "colab_type": "code",
        "outputId": "08da91ea-7414-4478-d111-f210937205df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Colab_Notebooks/rnn/wikitext'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.txt  train.txt  valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMNATVQ97D6C",
        "colab_type": "code",
        "outputId": "0a5bf808-65d2-453d-9a38-9bf9ea20f984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from torchtext.data import Field, BPTTIterator\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "\n",
        "def tokenizer(text): \n",
        "    return [i for i in text]\n",
        "\n",
        "batch_size = 128\n",
        "sequence_length = 30\n",
        "grad_clip = 0.1\n",
        "lr = 4.\n",
        "best_val_loss = None\n",
        "log_interval = 100\n",
        "\n",
        "text = Field(sequential=True, \n",
        "             tokenize=tokenizer)\n",
        "\n",
        "train_set, val_set, test_set =  WikiText2.splits(text_field=text,\n",
        "                                  train='/content/gdrive/My Drive/Colab_Notebooks/rnn/wikitext/train.txt',\n",
        "                                  validation='/content/gdrive/My Drive/Colab_Notebooks/rnn/wikitext/valid.txt',\n",
        "                                  test='/content/gdrive/My Drive/Colab_Notebooks/rnn/wikitext/test.txt')\n",
        "\n",
        "train_iter, val_iter, test_iter = BPTTIterator.splits(\n",
        "                                    (train_set, val_set, test_set),\n",
        "                                    batch_size=batch_size,\n",
        "                                    bptt_len=sequence_length,\n",
        "                                    repeat=False)\n",
        "\n",
        "text.build_vocab(train_set, val_set, test_set)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 41.4MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jISQyHIjd3XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        " \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        " \n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "        else:\n",
        "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq0UqQigOmeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(train_set.fields['text'].vocab.itos)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScVVL2E4ZEZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_tokens = len(train_set.fields['text'].vocab.itos)\n",
        "    for batch_idx, batch in enumerate(train_iter):\n",
        "        data, targets = batch.text, batch.target\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), torch.flatten(targets))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch_idx, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kcgEFIPRxf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        data, targets = batch.text, batch.target\n",
        "        output, hidden = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output.view(-1, ntokens), torch.flatten(targets)).item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mWQhMl_Q8oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(n=50, temp=1.):\n",
        "    model.eval()\n",
        "    x = torch.rand(1, 1).mul(ntokens).long()\n",
        "    hidden = None\n",
        "    out = []\n",
        "    for i in range(n):\n",
        "        output, hidden = model(x, hidden)\n",
        "        s_weights = output.squeeze().data.div(temp).exp()\n",
        "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
        "        x.data.fill_(s_idx)\n",
        "        s = train_set.fields['text'].vocab.itos[s_idx]\n",
        "        out.append(s)\n",
        "    return ''.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbx__VrvRsZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "423e7b3f-bea7-4c02-c094-4d4c4ee8e9dd"
      },
      "source": [
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_iter)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " èu्დć殻ვ﻿PنTاbاD,火لBtM隊€6ocGต1“pTA⅔ầH჻Á攻fF£D-♯्±هśN \n",
            "\n",
            "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.60 | ppl    36.68\n",
            "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.28 | ppl    26.67\n",
            "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.25 | ppl    25.80\n",
            "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.22 | ppl    25.15\n",
            "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.22 | ppl    24.98\n",
            "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.11 | ppl    22.43\n",
            "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  3.00 | ppl    20.16\n",
            "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.91 | ppl    18.44\n",
            "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.83 | ppl    16.89\n",
            "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.73 | ppl    15.27\n",
            "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.61 | ppl    13.66\n",
            "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.55 | ppl    12.82\n",
            "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.50 | ppl    12.20\n",
            "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.45 | ppl    11.62\n",
            "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.42 | ppl    11.24\n",
            "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.38 | ppl    10.85\n",
            "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.35 | ppl    10.46\n",
            "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.32 | ppl    10.18\n",
            "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.29 | ppl     9.92\n",
            "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.26 | ppl     9.59\n",
            "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.24 | ppl     9.40\n",
            "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.22 | ppl     9.16\n",
            "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.21 | ppl     9.09\n",
            "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.84\n",
            "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.73\n",
            "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.15 | ppl     8.57\n",
            "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.13 | ppl     8.42\n",
            "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss 57.83 | valid ppl 13037077740081885682335744.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " al meragte the hod crotut , Evon midearged bick an \n",
            "\n",
            "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.11 | ppl     8.29\n",
            "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.95\n",
            "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.87\n",
            "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.79\n",
            "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.68\n",
            "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.61\n",
            "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.54\n",
            "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.47\n",
            "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.42\n",
            "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.35\n",
            "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.25\n",
            "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.20\n",
            "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.15\n",
            "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
            "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.03\n",
            "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
            "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
            "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
            "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
            "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.79\n",
            "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
            "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.73\n",
            "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
            "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
            "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.61\n",
            "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.62\n",
            "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.57\n",
            "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss 49.98 | valid ppl 5104100928184657117184.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  ( <unk> and a . <eos> <eos> <eos> Cerma , which briting was f \n",
            "\n",
            "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.59\n",
            "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
            "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
            "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
            "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
            "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.31\n",
            "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.27\n",
            "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
            "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
            "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.21\n",
            "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.21\n",
            "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
            "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
            "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
            "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
            "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
            "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.08\n",
            "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
            "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
            "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.97\n",
            "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.96\n",
            "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.00\n",
            "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
            "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss 46.98 | valid ppl 253462148711257538560.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  , infruents .m. In k P the 26A3 Corerwanse , with \n",
            "\n",
            "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
            "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
            "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
            "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.84\n",
            "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
            "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
            "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
            "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
            "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
            "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
            "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
            "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
            "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
            "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
            "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
            "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
            "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
            "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
            "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
            "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss 45.47 | valid ppl 55928935020936970240.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ata with a arguer KPMDV+g“ , and ficty for may lea \n",
            "\n",
            "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
            "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
            "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
            "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
            "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
            "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
            "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
            "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
            "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
            "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
            "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss 44.57 | valid ppl 22825808293063159808.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " 30 he made sun in stilling @-@ bofh or generations \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0qPePkBYQqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = generate(10000, 1.)\n",
        "t15 = generate(10000, 1.5)\n",
        "t075 = generate(10000, 0.75)\n",
        "with open('/content/gdrive/My Drive/Colab_Notebooks/rnn/generated075.txt', 'w') as outf:\n",
        "    outf.write(t075)\n",
        "with open('/content/gdrive/My Drive/Colab_Notebooks/rnn/generated1.txt', 'w') as outf:\n",
        "    outf.write(t1)\n",
        "with open('/content/gdrive/My Drive/Colab_Notebooks/rnn/generated15.txt', 'w') as outf:\n",
        "    outf.write(t15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz2BpdWQYQq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}